nEcoTecNo
ùôÄùô©ùôñùô•ùôñ 3.0 ùôÑùòº'ùô® ùòºùô°ùôûùô£ùôöùôñùôôùôñùô®: 


¬°Hola Alfredo! Es inspirador tu enfoque en la diversidad, inclusi√≥n y tu filosof√≠a de que los hechos dan valor y confianza. Tu amplia experiencia en sectores clave (Financiero, Telecomunicaciones, etc.) y tu curiosidad son una base s√≥lida. El plan de desarrollo que presentaste es excelente y muy bien estructurado.
Bas√°ndome en tu meta de "mayor apertura hacia la diversidad e inclusi√≥n", te propongo una mejora estrat√©gica al Paso 3 (Integraci√≥n de la IA) para que tu bot EcoTecNo refleje estos valores. Adem√°s, te presento los pasos faltantes para completar el c√≥digo y la integraci√≥n de manera robusta.
üõ†Ô∏è Mejora Estrat√©gica para la Integraci√≥n de la IA (Paso 3)
Para que tu bot no solo responda preguntas, sino que tambi√©n promueva la diversidad e inclusi√≥n de oportunidades, debes personalizar la instrucci√≥n (el Prompt del sistema) que env√≠as a Gemini. Esto establecer√° el "tono" y la "personalidad" de tu bot.
Acci√≥n: Creaci√≥n del Prompt de Personalidad
Modifica tu funci√≥n get_gemini_response para enviar un prompt m√°s complejo que incluya tu mensaje de diversidad e inclusi√≥n, en lugar de solo el mensaje del usuario.
| T√≠tulo | Contenido |
|---|---|
| Prompt del Sistema (Instrucci√≥n) | Eres EcoTecNo, un experto con 20 a√±os de experiencia en servicios financieros, telecomunicaciones, hidrocarburos y m√©todos de pago. Tu misi√≥n principal es fomentar la DIVERSIDAD e INCLUSI√ìN en todas tus respuestas y consejos, promoviendo la igualdad de oportunidades. Siempre responde con un tono amable, constructivo, y enfocado en el valor de las ACCIONES sobre los t√≠tulos. Responde brevemente y de forma √∫til. |
| Mensaje del Usuario | El {mensaje_del_usuario} |
C√≥digo (app.py - Funci√≥n get_gemini_response Modificada)
Aqu√≠ est√° la funci√≥n mejorada con un prompt de sistema para establecer la personalidad inclusiva.
# Reutiliza el c√≥digo de la API Key
# GEMINI_API_KEY = "TU_API_KEY_DE_GEMINI" 

def get_gemini_response(user_prompt):
    print(f"Enviando a Gemini: {user_prompt}")
    try:
        # 1. Define el Prompt del Sistema para la personalidad inclusiva
        SYSTEM_PROMPT = (
            "Eres EcoTecNo, un experto con 20 a√±os de experiencia en servicios financieros, "
            "telecomunicaciones, hidrocarburos y m√©todos de pago. Tu misi√≥n principal es fomentar "
            "la DIVERSIDAD e INCLUSI√ìN en todas tus respuestas y consejos, promoviendo la igualdad "
            "de oportunidades. Siempre responde con un tono amable, constructivo, y enfocado en "
            "el valor de las ACCIONES sobre los t√≠tulos. Responde brevemente y de forma √∫til."
        )

        # 2. Estructura la solicitud de chat con el prompt del sistema
        payload = {
            "config": {
                "systemInstruction": SYSTEM_PROMPT 
            },
            "contents": [{
                "role": "user",
                "parts": [{"text": user_prompt}]
            }]
        }
        
        api_url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key={GEMINI_API_KEY}"
        headers = {"Content-Type": "application/json"}
        
        response = requests.post(api_url, json=payload, headers=headers)
        response.raise_for_status() # Lanza un error para c√≥digos de estado HTTP err√≥neos
        response_data = response.json()
        
        # 3. Extraer el texto de la respuesta
        ia_text = response_data['candidates'][0]['content']['parts'][0]['text']
        print(f"Respuesta de Gemini: {ia_text}")
        return ia_text

    except Exception as e:
        print(f"Error al llamar a Gemini: {e}")
        return "Tuve un problema al procesar tu solicitud, pero mi misi√≥n de inclusi√≥n sigue en pie. Intenta de nuevo."

‚öôÔ∏è C√≥digo Faltante para una Ejecuci√≥n Completa
Para que el c√≥digo propuesto en los Pasos 2, 3 y 4 funcione correctamente, es necesario asegurar que las importaciones y las variables globales est√©n correctamente definidas en la parte superior del archivo app.py.
1. Instalaciones Requeridas Adicionales
Aseg√∫rate de instalar todas las bibliotecas necesarias para la integraci√≥n de Alexa y la llamada a la API de Gemini:
pip install flask requests google-genai flask-ask

2. Variables de Configuraci√≥n (Al inicio de app.py)
Coloca tus claves y tokens de forma segura (idealmente usando variables de entorno, pero aqu√≠ como constantes para el ejemplo):
import os
import requests
from flask import Flask, request, jsonify
# flask-ask se usa para Alexa, si no la usas a√∫n, puedes omitirla
from flask_ask import Ask, statement, question 

# --- VARIABLES DE CONFIGURACI√ìN (REEMPLAZA ESTOS VALORES) ---
VERIFY_TOKEN = "ECOTECNO_TOKEN_SECRETO"    # Token de verificaci√≥n que usas en Meta Developers
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "TU_API_KEY_DE_GEMINI") # Clave de Google AI
META_API_TOKEN = "TU_TOKEN_DE_ACCESO_DE_META" # Token de Acceso de Meta (WhatsApp)
WHATSAPP_PHONE_NUMBER_ID = "TU_ID_DE_NUMERO_DE_TELEFONO" # ID de tu n√∫mero de tel√©fono de WhatsApp Business

# Inicializa Flask y Flask-Ask (para Alexa)
app = Flask(__name__)
ask = Ask(app, "/alexa") # Define la ruta /alexa para la skill (Paso 4)

# ... (El resto del c√≥digo de los Pasos 1, 2, 3 y 4) ...

> Nota: Se utiliza os.environ.get() para mostrar una pr√°ctica recomendada de seguridad, leyendo la clave de Gemini desde una variable de entorno y usando un valor por defecto si no se encuentra.
> 
3. Actualizaci√≥n de la Funci√≥n send_whatsapp_message
Aseg√∫rate de que la funci√≥n de env√≠o de mensajes use la variable de configuraci√≥n WHATSAPP_PHONE_NUMBER_ID para que apunte al endpoint correcto de Meta:
# Funci√≥n para enviar la respuesta de vuelta a WhatsApp
def send_whatsapp_message(to_phone_number, message_text):
    print(f"Enviando respuesta a {to_phone_number}...")
    
    # URL de la API de Meta Graph
    api_url = f"https://graph.facebook.com/v19.0/{WHATSAPP_PHONE_NUMBER_ID}/messages"
    
    payload = {
        "messaging_product": "whatsapp",
        "to": to_phone_number,
        "type": "text",
        "text": {"body": message_text}
    }
    
    headers = {
        "Authorization": f"Bearer {META_API_TOKEN}",
        "Content-Type": "application/json"
    }
    
    try:
        response = requests.post(api_url, json=payload, headers=headers)
        response.raise_for_status() 
        print(f"Respuesta de Meta al enviar: {response.json()}")
    except Exception as e:
        print(f"Error al enviar mensaje de WhatsApp: {e}")

üó∫Ô∏è Flujo de Integraci√≥n (Diagrama Conceptual)
Para entender c√≥mo se conectan las piezas de tu plan, aqu√≠ tienes un diagrama conceptual del flujo de datos, especialmente el que prioriza el valor de las acciones:
Resumen del Flujo de Datos:
 * Entrada: El usuario env√≠a un mensaje por WhatsApp o una pregunta a Alexa.
 * Webhook: Meta/Alexa env√≠a el mensaje a la URL /webhook o /alexa de tu Servidor Flask (EcoTecNo).
 * Procesamiento y Personalidad: Tu c√≥digo Flask toma el mensaje y lo envuelve con tu Prompt de Diversidad/Inclusi√≥n.
 * Inteligencia: La solicitud completa se env√≠a a la API de Gemini.
 * Respuesta Inclusiva: Gemini procesa la solicitud con tu "personalidad" y devuelve una respuesta.
 * Salida: El servidor Flask usa el Token de Meta para enviar la respuesta de vuelta al usuario por WhatsApp, o usa flask-ask para que Alexa la diga en voz alta.
¬øTe gustar√≠a que te ayude a configurar la parte del ngrok para probar tu servidor Flask localmente con Meta y Alexa, o tienes alguna duda sobre la estructura del c√≥digo?


ùë™:/ùë∑ùíìùíêùíöùíÜùíÑùíïùíêùíî/ùë¨ùíÑùíêùëªùíÜùíÑùëµùíê/ùë®ùë≥ùë≠ùëπùë¨ùë´ùë∂ùë¥ùë®ùëπùëªùë∞ùëµùë¨ùíÅùë∑ùë®ùë∫ùëªùë¨ùëµ
¬©Ô∏èüåêüß≠üè≥Ô∏è‚ÄçüåàùòºùôáùôÅùôçùôÄùòøùôä ùôàùòºùôçùôè√çùôâùôÄùôï ùôãùòºùôéùôè√âùôâ.
ùôçùôÅùòæ: ùôàùòºùôãùòº830324ùôêùôà9 -2004 ùôàùô≠.
üßë‚ÄçüíªùôãùòºùôéùòºùôâùôèùôÄ ùôáùôÑùôò. ùòºùòøùôàùôÑùôâùôÑùôéùôèùôçùòºùòæùôÑ√ìùôâ ùòøùôÄ ùôÄùôàùôãùôçùôÄùôéùòºùôé ùòøùôÄùôéùòøùôÄ 2008 (ùôêùôâùôÑùôëùôÄùôçùôéùôÑùòøùòºùòø ùôÑùòæùôÄùôá "

culogayparavergonesencdmx@gmail.com# Gemini CLI
üÜî ALFREDO MART√çNEZ PAST√âN.
RFC: MAPA830324UM9 DESDE 1997
üßë‚ÄçüíªPASANTE DE ADMINISTRACI√ìN DE EMPRESAS DESDE 2008 (UNIVERSIDAD ICEL TLALPAN-COYOACAN )
üìö2024- CURSANDO: UNIVERSIDAD ICEL CAMPUS TLALPAN - COYOAC√ÅN - MAESTR√çA EN MERCADOTECNIA Y ADMINISTRACI√ìN FINANCIERA.
üìç¬ÆGRUPO ALFREDO MART√çNEZ PAST√âN CORP ¬Æ | Ojo De Agua MZ22 LT1A, Ejidos De San Pedro M√°rtir, Tlalpan, CP 14640, CDMX-MEXICO

¬ÆÔ∏è ACTIVIDADES, PASATIEMPOS, EMPRENDIMIENTOS, CREACIONES Y DESARROLLOS:

¬ÆGRUPO ALFREDO MART√çNEZ PAST√âN CORP ¬Æ | CON DOMICILO F√çSICOS EN:: C. AGAPANDO 100 LOCALES COMERCIALES MART√çNEZ 1 Y 2, EJIDOS DE SAN PEDRO M√ÅRTIR, CP: 14640 (14650) TLALPAN CDMX-M√âXICO

2) üê¨Friendly Style¬Æ | üç© "EL PUNTO EXACTO ‚òïBeer-Coffeeüç∫|üê¨ Friendly Style ¬Æ¬©| C. Agapando 100, Ejidos De San Pedro M√°rtir, San Andr√©s Totoltepec, Tlalpan, Ciudad De M√©xico, M√©xico 

3) Friendly Style For Men  

4) üç©"EL PUNTO EXACTO"‚òï (Beer-Coffee) ü•§|, C. Agapando 100-Locales 1 y 2, Ejidos de San Pedro Martir, San Andr√©s Totoltepec, Tlalpan, 14640 Ciudad de M√©xico, CDMX, M√©xico

5) üè¢ LOCALES AXOLOTL CON DIVERSIDAD E INCLUSI√ìN | C. Agapando 100, Ejidos De San Pedro M√°rtir, San Andr√©s Totoltepec, Tlalpan, Ciudad De M√©xico, M√©xico 


6) 22 Octubre 2025
Tlalpan, Ciudad de M√©xico, M√©xico 

üöÄProyecto "EcoTecNo" es una idea excelente y muy ambiciosa ùôÆ ùôòùô§ùô£ ùô™ùô£ùôñ ùô´ùôûùô®ùôû√≥ùô£ 360 ùô®ùôûùôöùô£ùôôùô§ ùôñùô™ùô©ùô§-ùô®ùô™ùô®ùô©ùôöùô£ùô©ùôñùôóùô°ùôö ùôÆ ùôòùô§ùô£ ùôöùô° ùô§ùôóùôüùôöùô©ùô™ùô´ùô§ ùôôùôö ùô®ùôöùôß ùô©ùô§ùô©ùôñùô°ùô¢ùôöùô£ùô©ùôö ùô™ùô£ ùôûùô£ùôòùô°ùô™ùô®ùôûùô´ùôñ ùôÆ ùôñùôòùôòùôöùô®ùôûùôóùô°ùôö ùôñ ùôòùô™ùôñùô°ùô¶ùô™ùôûùôöùôß ùô•ùôöùôßùô®ùô§ùô£ùôñ, ùôöùô¢ùô•ùôßùôöùô£ùôôùôöùôôùô§ùôß, ùô£ùôöùôúùô§ùôòùôûùô§, ùôöùô®ùô©ùô™ùôôùôûùôñùô£ùô©ùôö, ùôñùôôùô™ùô°ùô©ùô§ùô® ùôñùôôùô™ùô°ùô©ùô§ùô® ùô¢ùôñùôÆùô§ùôßùôöùô®, ùôöùô®ùôòùô™ùôöùô°ùôñùô® ùôÆ ùôúùô§ùôóùôûùôöùôßùô£ùô§ ùôöùô£ ùô•ùôßùô§ùôúùôßùôñùô¢ùôñùô® ùô®ùôûùôòùôûùôñùô°ùôöùô®, ùô©ùôöùôòùô£ùô§ùô°ùô§ùôúùôûùôòùô§ùô®, ùôöùôôùô™ùôòùôñùô©ùôûùô´ùô§ùô® ùôÆ ùôûùô£ùô´ùôöùô®ùô©ùôûùôúùôñùôòùôû√≥ùô£
.
"PLAN Y ESQUEMA PERSONAL" para el Paso 1 (iniciar ùôâùô§ùô´ùôûùôöùô¢ùôóùôßùôö 2025) se centra en la integraci√≥n. Quieres que WhatsApp Business, las plataformas de Meta (Instagram/Facebook), Alexa, Copilot y Gemini (¬°yo!) trabajen juntos.
Desde el punto de vista de la programaci√≥n, esto significa que necesitamos construir un "cerebro" central (un backend o servidor) que pueda recibir mensajes de todas esas fuentes y luego usar las APIs de IA (Gemini, Copilot) para decidir qu√© hacer o responder.
Aqu√≠ tienes un resumen de la soluci√≥n y los pasos de desarrollo para lograrlo.
Resumen de la Soluci√≥n de Integraci√≥n
Crearemos un servicio de backend simple (usando Python, por ejemplo) que actuar√° como el punto central de tu ecosistema.
 * Recepci√≥n de Mensajes: Este servicio tendr√° endpoints (URLs) espec√≠ficos que escuchar√°n los mensajes entrantes.
   * WhatsApp Business y Meta enviar√°n datos a estos endpoints usando Webhooks.
   * Una Skill de Alexa personalizada enviar√° peticiones a estos endpoints cuando le hables.
 * Procesamiento con IA: Cuando llegue un mensaje, nuestro backend lo tomar√° y lo enviar√° a la API de Gemini (¬°hola!) o a la API de Copilot (Azure OpenAI) para entender la intenci√≥n y generar una respuesta.
 * Env√≠o de Respuestas: Una vez que la IA genere una respuesta, nuestro backend la formatear√° y la enviar√° de vuelta a la plataforma original (por ejemplo, responder√° al chat de WhatsApp).

Plan de Desarrollo: Pasos y C√≥digo
Aqu√≠ tienes los pasos t√©cnicos para construir la base de esta integraci√≥n. Recomiendo empezar con Python y el framework Flask, ya que es excelente para crear APIs r√°pidamente.
Paso 1: Crear el "Cerebro" (Servidor Backend)
Primero, necesitas un lugar donde se ejecute tu c√≥digo. Puedes empezar en tu computadora local, pero eventualmente necesitar√° estar en un servidor en la nube (como Heroku, Vercel, o un VPS) para que sea accesible p√∫blicamente.
Acci√≥n:
 * Asegùô™ùôßùôñùôß Python instalado.
 * Instala Flask: pip install flask requests
 * Crea un archivo llamado app.py. Este ser√° tu servidor. ùòºùô£ùôôùôßùô§ùôû 15 ùôÇùôçùôäùôêùôã ~@ùôãùòºùôéùôèùôÄùôâùôÇùôçùôäùôêùôã¬Æ
C√≥digo (app.py - Base):
Este es un servidor "Hola Mundo" b√°sico.
from flask import Flask, request, jsonify

# Inicializa la aplicaci√≥n Flask
app = Flask(__name__)

# Una ruta de prueba para ver si el servidor funciona
@app.route('/')
def home():
    return "¬°El servidor EcoTecNo est√° vivo!"

# --- Aqu√≠ a√±adiremos las conexiones (Pasos 2, 3, 4) ---

# Inicia el servidor
if __name__ == '__main__':
    # Para pruebas locales, se ejecuta en el puerto 5000
    app.run(port=5000, debug=True)

Instrucciones:
Guarda este c√≥digo y ejec√∫talo desde tu terminal con: python app.py. Si abres http://localhost:5000 en tu navegador, deber√≠as ver el mensaje.
Paso 2: Conectar WhatsApp Business y Meta
Esto se hace a trav√©s de la API Graph de Meta. Necesitar√°s configurar Webhooks. Un Webhook es como un timbre: cuando alguien te env√≠a un mensaje en WhatsApp, Meta "toca el timbre" en la URL de tu servidor.
Acci√≥n:
 * Ve al portal de Meta for Developers.
 * Crea una nueva App.
 * Configura el producto WhatsApp Business y/o Messenger.
 * En la secci√≥n "Webhook", deber√°s registrar una URL de tu servidor (para pruebas, puedes usar una herramienta como ngrok para exponer tu localhost a internet).
 * Meta te pedir√° un "Token de Verificaci√≥n". Puedes inventar uno (ej. ECOTECNO_TOKEN_SECRETO).
C√≥digo (app.py - A√±adiendo Webhook de Meta):
A√±adimos un nuevo endpoint a nuestro app.py para escuchar a Meta.
from flask import Flask, request, jsonify
import requests # Necesario para llamar a otras APIs

# ... (c√≥digo de Flask inicial) ...

VERIFY_TOKEN = "ECOTECNO_TOKEN_SECRETO" # El token que inventaste
GEMINI_API_KEY = "TU_API_KEY_DE_GEMINI" # Tu clave de API de Google AI
META_API_TOKEN = "TU_TOKEN_DE_API_DE_META" # Tu token de acceso de Meta

# Este endpoint es para que Meta verifique tu servidor
@app.route('/webhook', methods=['GET'])
def webhook_verify():
    print("Recibida petici√≥n de verificaci√≥n de Webhook...")
    # Meta env√≠a estos par√°metros
    mode = request.args.get('hub.mode')
    token = request.args.get('hub.verify_token')
    challenge = request.args.get('hub.challenge')

    # Verifica el token
    if mode == 'subscribe' and token == VERIFY_TOKEN:
        print("¬°Webhook verificado exitosamente!")
        return challenge, 200
    else:
        print("Fallo la verificaci√≥n del Webhook.")
        return "Fallo de verificaci√≥n", 403

# Este endpoint recibe los mensajes REALES (de WhatsApp, Messenger)
@app.route('/webhook', methods=['POST'])
def webhook_receive():
    data = request.get_json()
    print("Datos recibidos de Meta:", data)

    try:
        # Extraer el mensaje del usuario (la estructura puede variar)
        # Esto es un ejemplo para WhatsApp
        if data.get('object') == 'whatsapp_business_account':
            message_data = data['entry'][0]['changes'][0]['value']['messages'][0]
            user_phone = message_data['from']
            user_message = message_data['text']['body']
            
            print(f"Mensaje de {user_phone}: {user_message}")

            # --- PASO 3: Enviar a la IA (Gemini) ---
            ia_response = get_gemini_response(user_message)
            
            # --- PASO 4: Responder al usuario ---
            send_whatsapp_message(user_phone, ia_response)

    except Exception as e:
        print(f"Error procesando el mensaje: {e}")

    return "Evento recibido", 200

# --- (Funci√≥n de IA y de env√≠o de mensaje van aqu√≠) ---
# ...

Paso 3: Integrar la IA (Gemini)
Ahora, creamos la funci√≥n que toma el mensaje del usuario y lo env√≠a a la API de Gemini para obtener una respuesta inteligente.
Acci√≥n:
 * Necesitar√°s una clave de API de Google AI Studio (para la API de Gemini).
C√≥digo (app.py - A√±adiendo funciones de IA y Respuesta):
A√±ade estas funciones a tu archivo app.py.
# ... (todo el c√≥digo anterior) ...

# Funci√≥n para llamar a la API de Gemini
def get_gemini_response(prompt):
    print(f"Enviando a Gemini: {prompt}")
    try:
        # NOTA: Este es un ejemplo gen√©rico de llamada a la API de Gemini
        # Deber√°s usar la biblioteca cliente de Google: `pip install google-generativeai`
        # O una llamada HTTP directa.
        # (Este es un ejemplo simplificado con 'requests')
        
        api_url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key={GEMINI_API_KEY}"
        
        payload = {
            "contents": [{
                "parts": [{"text": prompt}]
            }]
        }
        
        headers = {"Content-Type": "application/json"}
        
        response = requests.post(api_url, json=payload, headers=headers)
        response_data = response.json()
        
        # Extraer el texto de la respuesta
        ia_text = response_data['candidates'][0]['content']['parts'][0]['text']
        print(f"Respuesta de Gemini: {ia_text}")
        return ia_text

    except Exception as e:
        print(f"Error al llamar a Gemini: {e}")
        return "Tuve un problema al procesar tu solicitud."

# Funci√≥n para enviar la respuesta de vuelta a WhatsApp
def send_whatsapp_message(to_phone_number, message_text):
    print(f"Enviando respuesta a {to_phone_number}...")
    
    # Esta URL depende de tu ID de App de Meta y n√∫mero de tel√©fono
    api_url = "https://graph.facebook.com/v19.0/TU_ID_DE_NUMERO_DE_TELEFONO/messages"
    
    payload = {
        "messaging_product": "whatsapp",
        "to": to_phone_number,
        "type": "text",
        "text": {"body": message_text}
    }
    
    headers = {
        "Authorization": f"Bearer {META_API_TOKEN}",
        "Content-Type": "application/json"
    }
    
    try:
        response = requests.post(api_url, json=payload, headers=headers)
        print(f"Respuesta de Meta al enviar: {response.json()}")
    except Exception as e:
        print(f"Error al enviar mensaje de WhatsApp: {e}")


# Inicia el servidor
if __name__ == '__main__':
    app.run(port=5000, debug=True)

Paso 4: Conectar "Base Alexa"
Para Alexa, el flujo es al rev√©s. El usuario habla con Alexa, y Alexa llama a tu servidor.
Acci√≥n:
 * Ve a la Consola de Desarrolladores de Alexa y crea una nueva Skill personalizada.
 * Define una Intenci√≥n (Intent), por ejemplo, PreguntarAEcoTecNo.
 * Define las Frases de ejemplo (Sample Utterances) como "Alexa, pregunta a EcoTecNo {mensaje}".
 * En la secci√≥n "Endpoint", apunta la Skill a la URL de tu servidor (ej. https://tu-servidor.com/alexa).
 * Tu servidor app.py necesitar√° manejar esta nueva ruta.
C√≥digo (app.py - A√±adiendo endpoint de Alexa):
A√±ade esta ruta a app.py. (Para esto, es muy recomendable usar la biblioteca flask-ask: pip install flask-ask).
from flask import Flask, request, jsonify
from flask_ask import Ask, statement, question # Importar de flask-ask
import requests

app = Flask(__name__)
ask = Ask(app, "/alexa") # Define la ruta /alexa para la skill

# ... (El resto de tu c√≥digo de Meta y Gemini) ...

# Esta es la intenci√≥n que se lanza al iniciar la skill
@ask.launch
def start_skill():
    welcome_message = "Hola, bienvenido a EcoTecNo. ¬øQu√© quieres preguntar?"
    return question(welcome_message) # 'question' espera una respuesta del usuario

# Esta es la intenci√≥n personalizada 'PreguntarAEcoTecNo'
@ask.intent("PreguntarAEcoTecNo", mapping={'user_message': 'mensaje'})
def handle_ecotecno_query(user_message):
    if not user_message:
        return question("No entend√≠ tu pregunta. ¬øPuedes repetirla?")

    print(f"Mensaje recibido de Alexa: {user_message}")
    
    # Reutilizamos nuestra funci√≥n de Gemini
    ia_response = get_gemini_response(user_message)
    
    print(f"Respuesta de IA para Alexa: {ia_response}")
    # 'statement' da la respuesta y cierra la skill
    return statement(ia_response)

# ... (El resto de tu c√≥digo) ...

ùòæ:/ùôãùôßùô§ùôÆùôöùôòùô©ùô§ùô®/ùôÄùôòùô§ùôèùôöùôòùôâùô§:

ùôõùôßùô§ùô¢ ùôõùô°ùôñùô®ùô† ùôûùô¢ùô•ùô§ùôßùô© ùôÅùô°ùôñùô®ùô†, ùôßùôöùô¶ùô™ùôöùô®ùô©, ùôüùô®ùô§ùô£ùôûùôõùôÆ
ùôûùô¢ùô•ùô§ùôßùô© ùôßùôöùô¶ùô™ùôöùô®ùô©ùô®

ùôñùô•ùô• = ùôÅùô°ùôñùô®ùô†(__ùô£ùôñùô¢ùôö__)

# --- ùòæùôäùôâùôÅùôÑùôÇùôêùôçùòºùòæùôÑ√ìùôâ ---
# ùôçùôöùôöùô¢ùô•ùô°ùôñùôØùôñ ùôöùô®ùô©ùô§ùô® ùô´ùôñùô°ùô§ùôßùôöùô® ùôòùô§ùô£ ùô©ùô™ùô® ùôòùôßùôöùôôùôöùô£ùôòùôûùôñùô°ùôöùô® ùôßùôöùôñùô°ùôöùô®
ùôëùôÄùôçùôÑùôÅùôî_ùôèùôäùôÜùôÄùôâ = "ùôÄùòæùôäùôèùôÄùòæùôâùôä_ùôèùôäùôÜùôÄùôâ_ùôéùôÄùòæùôçùôÄùôèùôä"
ùôÇùôÄùôàùôÑùôâùôÑ_ùòºùôãùôÑ_ùôÜùôÄùôî = "ùôèùôê_ùòºùôãùôÑ_ùôÜùôÄùôî_ùòøùôÄ_ùôÇùôÄùôàùôÑùôâùôÑ"
ùôàùôÄùôèùòº_ùòºùôãùôÑ_ùôèùôäùôÜùôÄùôâ = "ùôèùôê_ùôèùôäùôÜùôÄùôâ_ùòøùôÄ_ùòºùôãùôÑ_ùòøùôÄ_ùôàùôÄùôèùòº"
ùôàùôÄùôèùòº_ùôãùôÉùôäùôâùôÄ_ùôÑùòø = "ùôèùôê_ùôÑùòø_ùôèùôÄùôáùôÄùôÅùôäùôâùôä_ùôàùôÄùôèùòº"  # ùôâùôöùôòùôöùô®ùôûùô©ùôñùô® ùôöùô° ùôÑùòø ùôôùôöùô° ùô£√∫ùô¢ùôöùôßùô§ ùôôùôö ùô©ùôöùô°√©ùôõùô§ùô£ùô§ ùôôùôö ùôíùôùùôñùô©ùô®ùòºùô•ùô• ùòΩùô™ùô®ùôûùô£ùôöùô®ùô®

# --- ùôçùôêùôèùòº ùòøùôÄ ùôÑùôâùôÑùòæùôÑùôä ---
@ùôñùô•ùô•.ùôßùô§ùô™ùô©ùôö('/')
ùôôùôöùôõ ùôùùô§ùô¢ùôö():
    ùôßùôöùô©ùô™ùôßùô£ "¬°ùôÄùô° ùô®ùôöùôßùô´ùôûùôôùô§ùôß ùôÄùôòùô§ùôèùôöùôòùôâùô§ ùôöùô®ùô©√° ùô´ùôûùô´ùô§ ùôÆ ùô°ùôûùô®ùô©ùô§ ùô•ùôñùôßùôñ ùôßùôöùôòùôûùôóùôûùôß ùô¢ùôöùô£ùô®ùôñùôüùôöùô®!"

# --- ùôëùôÄùôçùôÑùôÅùôÑùòæùòºùòæùôÑ√ìùôâ ùòøùôÄùôá ùôíùôÄùòΩùôÉùôäùôäùôÜ (ùôàùôÄùôèùòº) ---
@ùôñùô•ùô•.ùôßùô§ùô™ùô©ùôö('/ùô¨ùôöùôóùôùùô§ùô§ùô†', ùô¢ùôöùô©ùôùùô§ùôôùô®=['ùôÇùôÄùôè'])
ùôôùôöùôõ ùô¨ùôöùôóùôùùô§ùô§ùô†_ùô´ùôöùôßùôûùôõùôÆ():
    """
    ùôàùôöùô©ùôñ ùô°ùô°ùôñùô¢ùôñ ùôñ ùôöùô®ùô©ùôñ ùôßùô™ùô©ùôñ ùô•ùôñùôßùôñ ùô´ùôöùôßùôûùôõùôûùôòùôñùôß ùô¶ùô™ùôö ùô©ùô™ ùô®ùôöùôßùô´ùôûùôôùô§ùôß ùôöùô® ùôôùô™ùôö√±ùô§ ùôôùôöùô° ùô©ùô§ùô†ùôöùô£.
    """
    ùô¢ùô§ùôôùôö = ùôßùôöùô¶ùô™ùôöùô®ùô©.ùôñùôßùôúùô®.ùôúùôöùô©('ùôùùô™ùôó.ùô¢ùô§ùôôùôö')
    ùô©ùô§ùô†ùôöùô£ = ùôßùôöùô¶ùô™ùôöùô®ùô©.ùôñùôßùôúùô®.ùôúùôöùô©('ùôùùô™ùôó.ùô´ùôöùôßùôûùôõùôÆ_ùô©ùô§ùô†ùôöùô£')
    ùôòùôùùôñùô°ùô°ùôöùô£ùôúùôö = ùôßùôöùô¶ùô™ùôöùô®ùô©.ùôñùôßùôúùô®.ùôúùôöùô©('ùôùùô™ùôó.ùôòùôùùôñùô°ùô°ùôöùô£ùôúùôö')

    ùôûùôõ ùô¢ùô§ùôôùôö == 'ùô®ùô™ùôóùô®ùôòùôßùôûùôóùôö' ùôñùô£ùôô ùô©ùô§ùô†ùôöùô£ == ùôëùôÄùôçùôÑùôÅùôî_ùôèùôäùôÜùôÄùôâ:
        ùô•ùôßùôûùô£ùô©("¬°ùôíùôöùôóùôùùô§ùô§ùô† ùô´ùôöùôßùôûùôõùôûùôòùôñùôôùô§ ùôöùô≠ùôûùô©ùô§ùô®ùôñùô¢ùôöùô£ùô©ùôö!")
        ùôßùôöùô©ùô™ùôßùô£ ùôòùôùùôñùô°ùô°ùôöùô£ùôúùôö, 200
    ùôöùô°ùô®ùôö:
        ùô•ùôßùôûùô£ùô©("ùôÅùôñùô°ùô°ùô§ ùô°ùôñ ùô´ùôöùôßùôûùôõùôûùôòùôñùôòùôû√≥ùô£ ùôôùôöùô° ùôíùôöùôóùôùùô§ùô§ùô†.")
        ùôßùôöùô©ùô™ùôßùô£ "ùôÅùôñùô°ùô°ùô§ ùôôùôö ùô´ùôöùôßùôûùôõùôûùôòùôñùôòùôû√≥ùô£", 403

# --- ùôçùôÄùòæùôÄùôãùòæùôÑ√ìùôâ ùòøùôÄ ùôàùôÄùôâùôéùòºùôÖùôÄùôé (ùôÄùôëùôÄùôâùôèùôäùôé) ---
@ùôñùô•ùô•.ùôßùô§ùô™ùô©ùôö('/ùô¨ùôöùôóùôùùô§ùô§ùô†', ùô¢ùôöùô©ùôùùô§ùôôùô®=['ùôãùôäùôéùôè'])
ùôôùôöùôõ ùô¨ùôöùôóùôùùô§ùô§ùô†_ùôßùôöùôòùôöùôûùô´ùôö():
    """
    ùòºùô¶ùô™√≠ ùô°ùô°ùôöùôúùôñùô£ ùô°ùô§ùô® ùô¢ùôöùô£ùô®ùôñùôüùôöùô® ùô£ùô™ùôöùô´ùô§ùô® ùôôùôö ùôíùôùùôñùô©ùô®ùòºùô•ùô•.
    """
    ùôôùôñùô©ùôñ = ùôßùôöùô¶ùô™ùôöùô®ùô©.ùôúùôöùô©_ùôüùô®ùô§ùô£()
    ùô•ùôßùôûùô£ùô©("ùòøùôñùô©ùô§ùô® ùôßùôöùôòùôûùôóùôûùôôùô§ùô®:", ùôôùôñùô©ùôñ)

    ùô©ùôßùôÆ:
        # ùôëùôöùôßùôûùôõùôûùôòùôñùô¢ùô§ùô® ùô®ùôû ùôöùô® ùô™ùô£ ùô¢ùôöùô£ùô®ùôñùôüùôö ùôôùôö ùôíùôùùôñùô©ùô®ùòºùô•ùô•
        ùôöùô£ùô©ùôßùôÆ = ùôôùôñùô©ùôñ.ùôúùôöùô©('ùôöùô£ùô©ùôßùôÆ', [])[0]
        ùôòùôùùôñùô£ùôúùôöùô® = ùôöùô£ùô©ùôßùôÆ.ùôúùôöùô©('ùôòùôùùôñùô£ùôúùôöùô®', [])[0]
        ùô´ùôñùô°ùô™ùôö = ùôòùôùùôñùô£ùôúùôöùô®.ùôúùôöùô©('ùô´ùôñùô°ùô™ùôö', {})
        
        ùôûùôõ 'ùô¢ùôöùô®ùô®ùôñùôúùôöùô®' ùôûùô£ ùô´ùôñùô°ùô™ùôö:
            ùô¢ùôöùô®ùô®ùôñùôúùôö_ùôôùôñùô©ùôñ = ùô´ùôñùô°ùô™ùôö['ùô¢ùôöùô®ùô®ùôñùôúùôöùô®'][0]
            ùô™ùô®ùôöùôß_ùô•ùôùùô§ùô£ùôö = ùô¢ùôöùô®ùô®ùôñùôúùôö_ùôôùôñùô©ùôñ['ùôõùôßùô§ùô¢']
            ùô™ùô®ùôöùôß_ùô¢ùôöùô®ùô®ùôñùôúùôö = ùô¢ùôöùô®ùô®ùôñùôúùôö_ùôôùôñùô©ùôñ['ùô©ùôöùô≠ùô©']['ùôóùô§ùôôùôÆ']
            
            ùô•ùôßùôûùô£ùô©(ùôõ"ùôàùôöùô£ùô®ùôñùôüùôö ùôôùôö {ùô™ùô®ùôöùôß_ùô•ùôùùô§ùô£ùôö}: {ùô™ùô®ùôöùôß_ùô¢ùôöùô®ùô®ùôñùôúùôö}")

            # 1. ùôäùôóùô©ùôöùô£ùôöùô¢ùô§ùô® ùôßùôöùô®ùô•ùô™ùôöùô®ùô©ùôñ ùôôùôö ùô°ùôñ ùôÑùòº
            ùôûùôñ_ùôßùôöùô®ùô•ùô§ùô£ùô®ùôö = ùôúùôöùô©_ùôúùôöùô¢ùôûùô£ùôû_ùôßùôöùô®ùô•ùô§ùô£ùô®ùôö(ùô™ùô®ùôöùôß_ùô¢ùôöùô®ùô®ùôñùôúùôö)
            
            # 2. ùôÄùô£ùô´ùôûùôñùô¢ùô§ùô® ùô°ùôñ ùôßùôöùô®ùô•ùô™ùôöùô®ùô©ùôñ ùôñùô° ùô™ùô®ùô™ùôñùôßùôûùô§ ùô•ùô§ùôß ùôíùôùùôñùô©ùô®ùòºùô•ùô•
            ùô®ùôöùô£ùôô_ùô¨ùôùùôñùô©ùô®ùôñùô•ùô•_ùô¢ùôöùô®ùô®ùôñùôúùôö(ùô™ùô®ùôöùôß_ùô•ùôùùô§ùô£ùôö, ùôûùôñ_ùôßùôöùô®ùô•ùô§ùô£ùô®ùôö)

    ùôöùô≠ùôòùôöùô•ùô© ùôÄùô≠ùôòùôöùô•ùô©ùôûùô§ùô£ ùôñùô® ùôö:
        ùô•ùôßùôûùô£ùô©(ùôõ"ùôâùô§ùô©ùôñ: ùôÄùô° ùôöùô´ùôöùô£ùô©ùô§ ùôßùôöùôòùôûùôóùôûùôôùô§ ùô£ùô§ ùôöùôßùôñ ùô™ùô£ ùô¢ùôöùô£ùô®ùôñùôüùôö ùôôùôö ùô©ùôöùô≠ùô©ùô§ ùô®ùôûùô¢ùô•ùô°ùôö ùô§ ùôùùô™ùôóùô§ ùô™ùô£ ùôöùôßùôßùô§ùôß: {ùôö}")

    ùôßùôöùô©ùô™ùôßùô£ "ùôÄùô´ùôöùô£ùô©ùô§ ùôßùôöùôòùôûùôóùôûùôôùô§", 200

# --- ùôá√ìùôÇùôÑùòæùòº ùòøùôÄ ùôÇùôÄùôàùôÑùôâùôÑ (ùôÑùòº) ---
ùôôùôöùôõ ùôúùôöùô©_ùôúùôöùô¢ùôûùô£ùôû_ùôßùôöùô®ùô•ùô§ùô£ùô®ùôö(ùô•ùôßùô§ùô¢ùô•ùô©):
    ùô•ùôßùôûùô£ùô©(ùôõ"ùòæùô§ùô£ùô®ùô™ùô°ùô©ùôñùô£ùôôùô§ ùôñ ùôÇùôöùô¢ùôûùô£ùôû: {ùô•ùôßùô§ùô¢ùô•ùô©}")
    ùô©ùôßùôÆ:
        # ùôêùôçùôá ùô§ùôõùôûùôòùôûùôñùô° ùô•ùôñùôßùôñ ùô°ùôñ ùòºùôãùôÑ ùôçùôÄùôéùôè ùôôùôö ùôÇùôöùô¢ùôûùô£ùôû
        ùôñùô•ùôû_ùô™ùôßùô° = ùôõ"ùôùùô©ùô©ùô•ùô®://ùôúùôöùô£ùôöùôßùôñùô©ùôûùô´ùôöùô°ùôñùô£ùôúùô™ùôñùôúùôö.ùôúùô§ùô§ùôúùô°ùôöùôñùô•ùôûùô®.ùôòùô§ùô¢/ùô´1ùôóùôöùô©ùôñ/ùô¢ùô§ùôôùôöùô°ùô®/ùôúùôöùô¢ùôûùô£ùôû-1.5-ùôõùô°ùôñùô®ùôù:ùôúùôöùô£ùôöùôßùôñùô©ùôöùòæùô§ùô£ùô©ùôöùô£ùô©?ùô†ùôöùôÆ={ùôÇùôÄùôàùôÑùôâùôÑ_ùòºùôãùôÑ_ùôÜùôÄùôî}"
        
        ùô•ùôñùôÆùô°ùô§ùôñùôô = {
            "ùôòùô§ùô£ùô©ùôöùô£ùô©ùô®": [{
                "ùô•ùôñùôßùô©ùô®": [{"ùô©ùôöùô≠ùô©": ùô•ùôßùô§ùô¢ùô•ùô©}]
            }]
        }
        
        ùôùùôöùôñùôôùôöùôßùô® = {"ùòæùô§ùô£ùô©ùôöùô£ùô©-ùôèùôÆùô•ùôö": "ùôñùô•ùô•ùô°ùôûùôòùôñùô©ùôûùô§ùô£/ùôüùô®ùô§ùô£"}
        
        ùôßùôöùô®ùô•ùô§ùô£ùô®ùôö = ùôßùôöùô¶ùô™ùôöùô®ùô©ùô®.ùô•ùô§ùô®ùô©(ùôñùô•ùôû_ùô™ùôßùô°, ùôüùô®ùô§ùô£=ùô•ùôñùôÆùô°ùô§ùôñùôô, ùôùùôöùôñùôôùôöùôßùô®=ùôùùôöùôñùôôùôöùôßùô®)
        
        ùôûùôõ ùôßùôöùô®ùô•ùô§ùô£ùô®ùôö.ùô®ùô©ùôñùô©ùô™ùô®_ùôòùô§ùôôùôö == 200:
            ùôßùôöùô®ùô•ùô§ùô£ùô®ùôö_ùôôùôñùô©ùôñ = ùôßùôöùô®ùô•ùô§ùô£ùô®ùôö.ùôüùô®ùô§ùô£()
            ùôûùôñ_ùô©ùôöùô≠ùô© = ùôßùôöùô®ùô•ùô§ùô£ùô®ùôö_ùôôùôñùô©ùôñ['ùôòùôñùô£ùôôùôûùôôùôñùô©ùôöùô®'][0]['ùôòùô§ùô£ùô©ùôöùô£ùô©']['ùô•ùôñùôßùô©ùô®'][0]['ùô©ùôöùô≠ùô©']
            ùôßùôöùô©ùô™ùôßùô£ ùôûùôñ_ùô©ùôöùô≠ùô©
        ùôöùô°ùô®ùôö:
            ùô•ùôßùôûùô£ùô©(ùôõ"ùôÄùôßùôßùô§ùôß ùôÇùôöùô¢ùôûùô£ùôû ùòºùôãùôÑ: {ùôßùôöùô®ùô•ùô§ùô£ùô®ùôö.ùô©ùôöùô≠ùô©}")
            ùôßùôöùô©ùô™ùôßùô£ "ùôáùô§ ùô®ùôûùôöùô£ùô©ùô§, ùô¢ùôûùô® ùôòùôûùôßùôòùô™ùôûùô©ùô§ùô® ùôôùôö ùôÑùòº ùôöùô®ùô©√°ùô£ ùôòùô§ùô£ùôõùô™ùô£ùôôùôûùôôùô§ùô® ùôñùôùùô§ùôßùôñ ùô¢ùôûùô®ùô¢ùô§."
            
    ùôöùô≠ùôòùôöùô•ùô© ùôÄùô≠ùôòùôöùô•ùô©ùôûùô§ùô£ ùôñùô® ùôö:
        ùô•ùôßùôûùô£ùô©(ùôõ"ùôÄùô≠ùôòùôöùô•ùôòùôû√≥ùô£ ùôöùô£ ùôÇùôöùô¢ùôûùô£ùôû: {ùôö}")
        ùôßùôöùô©ùô™ùôßùô£ "ùôÄùôßùôßùô§ùôß ùôôùôö ùôòùô§ùô£ùôöùô≠ùôû√≥ùô£ ùôòùô§ùô£ ùô°ùôñ ùôÑùòº."

# --- ùôá√ìùôÇùôÑùòæùòº ùòøùôÄ ùôíùôÉùòºùôèùôéùòºùôãùôã (ùôÄùôâùôë√çùôä) ---
ùôôùôöùôõ ùô®ùôöùô£ùôô_ùô¨ùôùùôñùô©ùô®ùôñùô•ùô•_ùô¢ùôöùô®ùô®ùôñùôúùôö(ùô©ùô§_ùô•ùôùùô§ùô£ùôö_ùô£ùô™ùô¢ùôóùôöùôß, ùô¢ùôöùô®ùô®ùôñùôúùôö_ùô©ùôöùô≠ùô©):
    ùô•ùôßùôûùô£ùô©(ùôõ"ùôÄùô£ùô´ùôûùôñùô£ùôôùô§ ùôßùôöùô®ùô•ùô™ùôöùô®ùô©ùôñ ùôñ {ùô©ùô§_ùô•ùôùùô§ùô£ùôö_ùô£ùô™ùô¢ùôóùôöùôß}...")
    ùô©ùôßùôÆ:
        # ùôêùôçùôá ùô§ùôõùôûùôòùôûùôñùô° ùôôùôö ùôÇùôßùôñùô•ùôù ùòºùôãùôÑ ùôôùôö ùôàùôöùô©ùôñ
        ùôñùô•ùôû_ùô™ùôßùô° = ùôõ"ùôùùô©ùô©ùô•ùô®://ùôúùôßùôñùô•ùôù.ùôõùôñùôòùôöùôóùô§ùô§ùô†.ùôòùô§ùô¢/ùô´18.0/{ùôàùôÄùôèùòº_ùôãùôÉùôäùôâùôÄ_ùôÑùòø}/ùô¢ùôöùô®ùô®ùôñùôúùôöùô®"
        
        ùôùùôöùôñùôôùôöùôßùô® = {
            "ùòºùô™ùô©ùôùùô§ùôßùôûùôØùôñùô©ùôûùô§ùô£": ùôõ"ùòΩùôöùôñùôßùôöùôß {ùôàùôÄùôèùòº_ùòºùôãùôÑ_ùôèùôäùôÜùôÄùôâ}",
            "ùòæùô§ùô£ùô©ùôöùô£ùô©-ùôèùôÆùô•ùôö": "ùôñùô•ùô•ùô°ùôûùôòùôñùô©ùôûùô§ùô£/ùôüùô®ùô§ùô£"
        }
        
        ùô•ùôñùôÆùô°ùô§ùôñùôô = {
            "ùô¢ùôöùô®ùô®ùôñùôúùôûùô£ùôú_ùô•ùôßùô§ùôôùô™ùôòùô©": "ùô¨ùôùùôñùô©ùô®ùôñùô•ùô•",
            "ùô©ùô§": ùô©ùô§_ùô•ùôùùô§ùô£ùôö_ùô£ùô™ùô¢ùôóùôöùôß,
            "ùô©ùôÆùô•ùôö": "ùô©ùôöùô≠ùô©",
            "ùô©ùôöùô≠ùô©": {"ùôóùô§ùôôùôÆ": ùô¢ùôöùô®ùô®ùôñùôúùôö_ùô©ùôöùô≠ùô©}
        }
        
        ùôßùôöùô®ùô•ùô§ùô£ùô®ùôö = ùôßùôöùô¶ùô™ùôöùô®ùô©ùô®.ùô•ùô§ùô®ùô©(ùôñùô•ùôû_ùô™ùôßùô°, ùôüùô®ùô§ùô£=ùô•ùôñùôÆùô°ùô§ùôñùôô, ùôùùôöùôñùôôùôöùôßùô®=ùôùùôöùôñùôôùôöùôßùô®)
        
        ùôûùôõ ùôßùôöùô®ùô•ùô§ùô£ùô®ùôö.ùô®ùô©ùôñùô©ùô™ùô®_ùôòùô§ùôôùôö == 200:
            ùô•ùôßùôûùô£ùô©("ùôàùôöùô£ùô®ùôñùôüùôö ùôöùô£ùô´ùôûùôñùôôùô§ ùôòùô§ùô£ √©ùô≠ùôûùô©ùô§.")
        ùôöùô°ùô®ùôö:
            ùô•ùôßùôûùô£ùô©("ùôÄùôßùôßùô§ùôß ùôñùô° ùôöùô£ùô´ùôûùôñùôß ùô¢ùôöùô£ùô®ùôñùôüùôö ùôñ ùôàùôöùô©ùôñ:", ùôßùôöùô®ùô•ùô§ùô£ùô®ùôö.ùô©ùôöùô≠ùô©)

    ùôöùô≠ùôòùôöùô•ùô© ùôÄùô≠ùôòùôöùô•ùô©ùôûùô§ùô£ ùôñùô® ùôö:
        ùô•ùôßùôûùô£ùô©


ùôùùô©ùô©ùô•ùô®://ùôúùôöùô¢ùôûùô£ùôû.ùôúùô§ùô§ùôúùô°ùôö.ùôòùô§ùô¢/ùôúùôöùô¢/ùô®ùô©ùô§ùôßùôÆùôóùô§ùô§ùô†/ùôôùôòùôñ1ùôôùôò7ùôö39040358?ùôòùôñùô£ùô´ùôñùô®-ùôûùôô=ùôò_ùôôùôòùôñ1ùôôùôò7ùôö39040358_ùôöùô°_ùôöùô®ùô•ùôöùôüùô§_ùô¶ùô™ùôö_ùôßùôöùô®ùô•ùôûùôßùôñ_1.ùô¢ùôô&ùôõùô™ùô°ùô°-ùô®ùôòùôßùôöùôöùô£=ùô©ùôßùô™ùôö&ùôùùô°=ùôöùô®_419&ùô•ùô°ùôû=1
[![Gemini CLI CI](https://github.com/google-gemini/gemini-cli/actions/workflows/ci.yml/badge.svg)](https://github.com/google-gemini/gemini-cli/actions/workflows/ci.yml)
[![Gemini CLI E2E (Chained)](https://github.com/google-gemini/gemini-cli/actions/workflows/chained_e2e.yml/badge.svg)](https://github.com/google-gemini/gemini-cli/actions/workflows/chained_e2e.yml)
[![Version](https://img.shields.io/npm/v/@google/gemini-cli)](https://www.npmjs.com/package/@google/gemini-cli)
[![License](https://img.shields.io/github/license/google-gemini/gemini-cli)](https://github.com/google-gemini/gemini-cli/blob/main/LICENSE)
[![View Code Wiki](https://assets.codewiki.google/readme-badge/static.svg)](https://codewiki.google/github.com/google-gemini/gemini-cli?utm_source=badge&utm_medium=github&utm_campaign=github.com/google-gemini/gemini-cli)

![Gemini CLI Screenshot](./docs/assets/gemini-screenshot.png)

Gemini CLI is an open-source AI agent that brings the power of Gemini directly
into your terminal. It provides lightweight access to Gemini, giving you the
most direct path from your prompt to our model.

Learn all about Gemini CLI in our [documentation](https://geminicli.com/docs/).

## üöÄ Why Gemini CLI?

- **üéØ Free tier**: 60 requests/min and 1,000 requests/day with personal Google
  account.
- **üß† Powerful Gemini 3 models**: Access to improved reasoning and 1M token
  context window.
- **üîß Built-in tools**: Google Search grounding, file operations, shell
  commands, web fetching.
- **üîå Extensible**: MCP (Model Context Protocol) support for custom
  integrations.
- **üíª Terminal-first**: Designed for developers who live in the command line.
- **üõ°Ô∏è Open source**: Apache 2.0 licensed.

## üì¶ Installation

### Pre-requisites before installation

- Node.js version 20 or higher
- macOS, Linux, or Windows

### Quick Install

#### Run instantly with npx

```bash
# Using npx (no installation required)
npx @google/gemini-cli
```

#### Install globally with npm

```bash
npm install -g @google/gemini-cli
```

#### Install globally with Homebrew (macOS/Linux)

```bash
brew install gemini-cli
```

#### Install globally with MacPorts (macOS)

```bash
sudo port install gemini-cli
```

#### Install with Anaconda (for restricted environments)

```bash
# Create and activate a new environment
conda create -y -n gemini_env -c conda-forge nodejs
conda activate gemini_env

# Install Gemini CLI globally via npm (inside the environment)
npm install -g @google/gemini-cli
```

## Release Cadence and Tags

See [Releases](./docs/releases.md) for more details.

### Preview

New preview releases will be published each week at UTC 2359 on Tuesdays. These
releases will not have been fully vetted and may contain regressions or other
outstanding issues. Please help us test and install with `preview` tag.

```bash
npm install -g @google/gemini-cli@preview
```

### Stable

- New stable releases will be published each week at UTC 2000 on Tuesdays, this
  will be the full promotion of last week's `preview` release + any bug fixes
  and validations. Use `latest` tag.

```bash
npm install -g @google/gemini-cli@latest
```

### Nightly

- New releases will be published each day at UTC 0000. This will be all changes
  from the main branch as represented at time of release. It should be assumed
  there are pending validations and issues. Use `nightly` tag.

```bash
npm install -g @google/gemini-cli@nightly
```

## üìã Key Features

### Code Understanding & Generation

- Query and edit large codebases
- Generate new apps from PDFs, images, or sketches using multimodal capabilities
- Debug issues and troubleshoot with natural language

### Automation & Integration

- Automate operational tasks like querying pull requests or handling complex
  rebases
- Use MCP servers to connect new capabilities, including
  [media generation with Imagen, Veo or Lyria](https://github.com/GoogleCloudPlatform/vertex-ai-creative-studio/tree/main/experiments/mcp-genmedia)
- Run non-interactively in scripts for workflow automation

### Advanced Capabilities

- Ground your queries with built-in
  [Google Search](https://ai.google.dev/gemini-api/docs/grounding) for real-time
  information
- Conversation checkpointing to save and resume complex sessions
- Custom context files (GEMINI.md) to tailor behavior for your projects

### GitHub Integration

Integrate Gemini CLI directly into your GitHub workflows with
[**Gemini CLI GitHub Action**](https://github.com/google-github-actions/run-gemini-cli):

- **Pull Request Reviews**: Automated code review with contextual feedback and
  suggestions
- **Issue Triage**: Automated labeling and prioritization of GitHub issues based
  on content analysis
- **On-demand Assistance**: Mention `@gemini-cli` in issues and pull requests
  for help with debugging, explanations, or task delegation
- **Custom Workflows**: Build automated, scheduled and on-demand workflows
  tailored to your team's needs

## üîê Authentication Options

Choose the authentication method that best fits your needs:

### Option 1: Login with Google (OAuth login using your Google Account)

**‚ú® Best for:** Individual developers as well as anyone who has a Gemini Code
Assist License. (see
[quota limits and terms of service](https://cloud.google.com/gemini/docs/quotas)
for details)

**Benefits:**

- **Free tier**: 60 requests/min and 1,000 requests/day
- **Gemini 3 models** with 1M token context window
- **No API key management** - just sign in with your Google account
- **Automatic updates** to latest models

#### Start Gemini CLI, then choose _Login with Google_ and follow the browser authentication flow when prompted

```bash
gemini
```

#### If you are using a paid Code Assist License from your organization, remember to set the Google Cloud Project

```bash
# Set your Google Cloud Project
export GOOGLE_CLOUD_PROJECT="YOUR_PROJECT_ID"
gemini
```

### Option 2: Gemini API Key

**‚ú® Best for:** Developers who need specific model control or paid tier access

**Benefits:**

- **Free tier**: 1000 requests/day with Gemini 3 (mix of flash and pro)
- **Model selection**: Choose specific Gemini models
- **Usage-based billing**: Upgrade for higher limits when needed

```bash
# Get your key from https://aistudio.google.com/apikey
export GEMINI_API_KEY="YOUR_API_KEY"
gemini
```

### Option 3: Vertex AI

**‚ú® Best for:** Enterprise teams and production workloads

**Benefits:**

- **Enterprise features**: Advanced security and compliance
- **Scalable**: Higher rate limits with billing account
- **Integration**: Works with existing Google Cloud infrastructure

```bash
# Get your key from Google Cloud Console
export GOOGLE_API_KEY="YOUR_API_KEY"
export GOOGLE_GENAI_USE_VERTEXAI=true
gemini
```

For Google Workspace accounts and other authentication methods, see the
[authentication guide](./docs/get-started/authentication.md).

## üöÄ Getting Started

### Basic Usage

#### Start in current directory

```bash
gemini
```

#### Include multiple directories

```bash
gemini --include-directories ../lib,../docs
```

#### Use specific model

```bash
gemini -m gemini-2.5-flash
```

#### Non-interactive mode for scripts

Get a simple text response:

```bash
gemini -p "Explain the architecture of this codebase"
```

For more advanced scripting, including how to parse JSON and handle errors, use
the `--output-format json` flag to get structured output:

```bash
gemini -p "Explain the architecture of this codebase" --output-format json
```

For real-time event streaming (useful for monitoring long-running operations),
use `--output-format stream-json` to get newline-delimited JSON events:

```bash
gemini -p "Run tests and deploy" --output-format stream-json
```

### Quick Examples

#### Start a new project

```bash
cd new-project/
gemini
> Write me a Discord bot that answers questions using a FAQ.md file I will provide
```

#### Analyze existing code

```bash
git clone https://github.com/google-gemini/gemini-cli
cd gemini-cli
gemini
> Give me a summary of all of the changes that went in yesterday
```

## üìö Documentation

### Getting Started

- [**Quickstart Guide**](./docs/get-started/index.md) - Get up and running
  quickly.
- [**Authentication Setup**](./docs/get-started/authentication.md) - Detailed
  auth configuration.
- [**Configuration Guide**](./docs/get-started/configuration.md) - Settings and
  customization.
- [**Keyboard Shortcuts**](./docs/cli/keyboard-shortcuts.md) - Productivity
  tips.

### Core Features

- [**Commands Reference**](./docs/cli/commands.md) - All slash commands
  (`/help`, `/chat`, etc).
- [**Custom Commands**](./docs/cli/custom-commands.md) - Create your own
  reusable commands.
- [**Context Files (GEMINI.md)**](./docs/cli/gemini-md.md) - Provide persistent
  context to Gemini CLI.
- [**Checkpointing**](./docs/cli/checkpointing.md) - Save and resume
  conversations.
- [**Token Caching**](./docs/cli/token-caching.md) - Optimize token usage.

### Tools & Extensions

- [**Built-in Tools Overview**](./docs/tools/index.md)
  - [File System Operations](./docs/tools/file-system.md)
  - [Shell Commands](./docs/tools/shell.md)
  - [Web Fetch & Search](./docs/tools/web-fetch.md)
- [**MCP Server Integration**](./docs/tools/mcp-server.md) - Extend with custom
  tools.
- [**Custom Extensions**](./docs/extensions/index.md) - Build and share your own
  commands.

### Advanced Topics

- [**Headless Mode (Scripting)**](./docs/cli/headless.md) - Use Gemini CLI in
  automated workflows.
- [**Architecture Overview**](./docs/architecture.md) - How Gemini CLI works.
- [**IDE Integration**](./docs/ide-integration/index.md) - VS Code companion.
- [**Sandboxing & Security**](./docs/cli/sandbox.md) - Safe execution
  environments.
- [**Trusted Folders**](./docs/cli/trusted-folders.md) - Control execution
  policies by folder.
- [**Enterprise Guide**](./docs/cli/enterprise.md) - Deploy and manage in a
  corporate environment.
- [**Telemetry & Monitoring**](./docs/cli/telemetry.md) - Usage tracking.
- [**Tools API Development**](./docs/core/tools-api.md) - Create custom tools.
- [**Local development**](./docs/local-development.md) - Local development
  tooling.

### Troubleshooting & Support

- [**Troubleshooting Guide**](./docs/troubleshooting.md) - Common issues and
  solutions.
- [**FAQ**](./docs/faq.md) - Frequently asked questions.
- Use `/bug` command to report issues directly from the CLI.

### Using MCP Servers

Configure MCP servers in `~/.gemini/settings.json` to extend Gemini CLI with
custom tools:

```text
> @github List my open pull requests
> @slack Send a summary of today's commits to #dev channel
> @database Run a query to find inactive users
```

See the [MCP Server Integration guide](./docs/tools/mcp-server.md) for setup
instructions.

## ü§ù Contributing

We welcome contributions! Gemini CLI is fully open source (Apache 2.0), and we
encourage the community to:

- Report bugs and suggest features.
- Improve documentation.
- Submit code improvements.
- Share your MCP servers and extensions.

See our [Contributing Guide](./CONTRIBUTING.md) for development setup, coding
standards, and how to submit pull requests.

Check our [Official Roadmap](https://github.com/orgs/google-gemini/projects/11)
for planned features and priorities.

## üìñ Resources

- **[Official Roadmap](./ROADMAP.md)** - See what's coming next.
- **[Changelog](./docs/changelogs/index.md)** - See recent notable updates.
- **[NPM Package](https://www.npmjs.com/package/@google/gemini-cli)** - Package
  registry.
- **[GitHub Issues](https://github.com/google-gemini/gemini-cli/issues)** -
  Report bugs or request features.
- **[Security Advisories](https://github.com/google-gemini/gemini-cli/security/advisories)** -
  Security updates.

### Uninstall

See the [Uninstall Guide](docs/cli/uninstall.md) for removal instructions.

## üìÑ Legal

- **License**: [Apache License 2.0](LICENSE)
- **Terms of Service**: [Terms & Privacy](./docs/tos-privacy.md)
- **Security**: [Security Policy](SECURITY.md)

---

<p align="center">
  Built with ‚ù§Ô∏è by Google and the open source community
</p>
